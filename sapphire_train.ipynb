{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f5b893-79d0-4214-9edb-29cf64498a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from libworm.torch.gamma_net import GammaNeuronNet, from_connectome\n",
    "from libworm.data import connectomes, traces\n",
    "from libworm import preprocess\n",
    "from libworm.functions import set_neurons, tcalc_s_inf, set_trace\n",
    "#from libworm.training import basic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a17e92-9e7e-4cab-a486-4fa66262df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_train(model, criterion, optimizer, points, labels,\n",
    "          cell_labels, epoches=5, batch=64,\n",
    "          timestep=0.001, data_timestep=0.60156673, do_print=True):\n",
    "\n",
    "    dataset = TensorDataset(points, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    loss = -1\n",
    "    \n",
    "    for i in range(1,epoches+1):\n",
    "        for points_batch, labels_batch in dataloader:\n",
    "\n",
    "            points_batch = torch.squeeze(points_batch)\n",
    "            labels_batch = torch.squeeze(labels_batch)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            results = model(points_batch, timestep, data_timestep)\n",
    "\n",
    "            padded_labels = F.pad(labels_batch,\n",
    "                                  (0, results.shape[0] - labels_batch.shape[0]),\n",
    "                                  \"constant\",\n",
    "                                  0)\n",
    "            #Compare\n",
    "            loss = criterion(results, padded_labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f\"Batch Complete: Time {start_time - end_time} Loss: {loss.item()}\")\n",
    " \n",
    "    total_end_time = time.time()\n",
    "    total_time_taken = total_end_time - total_start_time\n",
    "    if(do_print):\n",
    "        print(f\"Total Time {total_time_taken}\")\n",
    "        \n",
    "    final_loss = loss.item()\n",
    "    \n",
    "    return (total_time_taken, final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317ea828-6b35-4389-b592-f92a8bee04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4687)\n",
    "_, trace, trace_labels, label2index, timestamps = traces.load_trace()\n",
    "timestamps = timestamps - timestamps[0]\n",
    "\n",
    "chemical, gapjn = connectomes.load_cook_connectome()\n",
    "neurons = connectomes.get_main_neurons(chemical, gapjn)\n",
    "neurons.sort(key=lambda item: f\"AAA{label2index[item]:04d}{item}\" if item in label2index else item)\n",
    "model = from_connectome(chemical, gapjn, neurons)\n",
    "\n",
    "not_in_main_section = [label2index[key] for key in label2index if key not in neurons]\n",
    "not_labelled = [i for i, _ in enumerate(trace[:, 0]) if i not in label2index.values()]\n",
    "\n",
    "removal = list(set(not_in_main_section).union(not_labelled))\n",
    "\n",
    "trace = np.delete(trace, removal, axis=0)\n",
    "\n",
    "points, labels = preprocess.window_split(trace, window_size = 2, points_size = 1)\n",
    "\n",
    "points = torch.from_numpy(points.squeeze())\n",
    "labels = torch.from_numpy(labels.squeeze())\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(points, labels, train_size=0.1)\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "crit = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f9b044-8ce2-46d9-af75-76339980aa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4510, -4.1009, -2.2121, -2.8197, -3.5807, -3.4202, -4.2683, -2.8373,\n",
       "        -2.5259, -2.4097, -2.1104, -2.5873, -1.9682, -3.0283, -2.6589, -2.2539,\n",
       "        -2.7347, -2.7286, -5.6825, -3.9146, -3.7411, -2.9963, -1.9164, -3.4260,\n",
       "        -3.4760, -3.2163, -2.7737, -2.5176, -2.6184, -5.1174, -3.3332, -3.1200,\n",
       "        -3.4981, -2.8818, -2.6268, -3.8619, -3.6338, -3.9215, -4.7184, -2.7087,\n",
       "        -5.0857, -2.8634, -3.0128, -2.2239, -3.1470, -2.3011, -2.7978, -1.7358,\n",
       "        -3.7177, -2.8540, -2.3609, -3.3702, -2.1525, -3.3201, -3.6280, -2.7761,\n",
       "        -2.7870, -6.2958, -2.2453, -2.7324, -1.8643, -3.7711, -2.2164, -2.5862,\n",
       "        -2.4835, -3.3272, -2.2202, -4.0398, -3.3971, -2.5025, -3.3704, -3.2671,\n",
       "        -5.6311, -5.9084, -3.0096, -2.7314, -2.8248, -2.9190, -2.4125, -3.0189,\n",
       "        -2.7221, -3.1844, -3.3156, -4.3993, -6.3268, -5.6502, -5.8799, -2.7060,\n",
       "        -3.8920, -2.5007, -2.0515, -4.6782, -2.4053, -3.1120, -4.3757, -3.3757,\n",
       "        -3.3083, -2.3781, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "       dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(points[0], 0.01, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d07763-9116-4d45-a936-33536fb427d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Complete: Time -6.1863157749176025 Loss: 5.127374925191802\n",
      "Batch Complete: Time -6.338157653808594 Loss: 3.2899625125327816\n",
      "Batch Complete: Time -5.987959146499634 Loss: 3.432591177443225\n",
      "Batch Complete: Time -6.43522834777832 Loss: 2.7273574983410125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbasic_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneurons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepoches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mbasic_train\u001b[0;34m(model, criterion, optimizer, points, labels, cell_labels, epoches, batch, timestep, data_timestep, do_print)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#Compare\u001b[39;00m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(results, padded_labels)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = basic_train(model, crit, optimiser,\n",
    "                train_x, train_y, neurons,\n",
    "                epoches=2, batch=6, timestep=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d54283a-5755-4540-8f33-a7a96d6b9249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 9.9960,  9.9960,  9.9960,  9.9960,  9.9960,  9.9961,  9.9960,  9.9961,\n",
      "         9.9961,  9.9960,  9.9960,  9.9962,  9.9960,  9.9961,  9.9960,  9.9960,\n",
      "         9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9961,  9.9963,  9.9962,\n",
      "         9.9961,  9.9960,  9.9960,  9.9961,  9.9961,  9.9960,  9.9962,  9.9961,\n",
      "         9.9961,  9.9961,  9.9961,  9.9961,  9.9960,  9.9960,  9.9961,  9.9961,\n",
      "         9.9960,  9.9961,  9.9961,  9.9961,  9.9961,  9.9963,  9.9962,  9.9963,\n",
      "         9.9961,  9.9960,  9.9961,  9.9961,  9.9961,  9.9960,  9.9961,  9.9962,\n",
      "         9.9960,  9.9960,  9.9960,  9.9960,  9.9963,  9.9960,  9.9960,  9.9960,\n",
      "         9.9962,  9.9962,  9.9960,  9.9961,  9.9960,  9.9960,  9.9961,  9.9960,\n",
      "         9.9960,  9.9960,  9.9961,  9.9961,  9.9960,  9.9960,  9.9961,  9.9962,\n",
      "         9.9961,  9.9960,  9.9960,  9.9961,  9.9960,  9.9961,  9.9960,  9.9960,\n",
      "         9.9962,  9.9961,  9.9962,  9.9961,  9.9962,  9.9962,  9.9960,  9.9960,\n",
      "         9.9962,  9.9961,  9.9961,  9.9961,  9.9961,  9.9960,  9.9960,  9.9960,\n",
      "         9.9961,  9.9961,  9.9960,  9.9960,  9.9961,  9.9961,  9.9960,  9.9960,\n",
      "         9.9961,  9.9960,  9.9961,  9.9960,  9.9960,  9.9960,  9.9961,  9.9961,\n",
      "         9.9960,  9.9960,  9.9975,  9.9971,  9.9981,  9.9960,  9.9960,  9.9960,\n",
      "         9.9961,  9.9961,  9.9960,  9.9961,  9.9960,  9.9960,  9.9960,  9.9960,\n",
      "         9.9960,  9.9960,  9.9961,  9.9961,  9.9960,  9.9960,  9.9960,  9.9960,\n",
      "         9.9960,  9.9960,  9.9960,  9.9961,  9.9961,  9.9961,  9.9961,  9.9961,\n",
      "         9.9960,  9.9961,  9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960,\n",
      "         9.9960,  9.9960,  9.9960,  9.9961,  9.9961,  9.9960,  9.9960,  9.9960,\n",
      "         9.9960,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960,\n",
      "         9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960,  9.9961,  9.9961,\n",
      "         9.9960,  9.9961,  9.9961,  9.9961,  9.9961,  9.9961,  9.9961,  9.9961,\n",
      "         9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9961,  9.9961,  9.9961,\n",
      "         9.9960,  9.9961,  9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960,\n",
      "         9.9960,  9.9961,  9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9961,\n",
      "         9.9960,  9.9961,  9.9961,  9.9960,  9.9961,  9.9960,  9.9961,  9.9960,\n",
      "         9.9961,  9.9961,  9.9961,  9.9961,  9.9960,  9.9960,  9.9961,  9.9961,\n",
      "         9.9961,  9.9960,  9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9961,\n",
      "         9.9961,  9.9961,  9.9960,  9.9961,  9.9961,  9.9961,  9.9961,  9.9961,\n",
      "         9.9961,  9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960,\n",
      "         9.9960,  9.9960,  9.9960,  9.9960,  9.9960,  9.9961,  9.9960,  9.9960,\n",
      "         9.9960,  9.9960,  9.9960,  9.9960,  9.9961,  9.9962, 10.0005, 10.0039,\n",
      "         9.9960,  9.9961,  9.9961,  9.9960,  9.9960,  9.9960,  9.9960,  9.9960],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9961, -34.9960,\n",
      "        -34.9961, -34.9961, -34.9960, -34.9960, -34.9962, -34.9960, -34.9961,\n",
      "        -34.9960, -34.9960, -34.9961, -34.9960, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9961, -34.9963, -34.9962, -34.9961, -34.9960, -34.9960, -34.9961,\n",
      "        -34.9961, -34.9960, -34.9962, -34.9961, -34.9961, -34.9961, -34.9961,\n",
      "        -34.9961, -34.9960, -34.9960, -34.9961, -34.9961, -34.9960, -34.9961,\n",
      "        -34.9961, -34.9961, -34.9961, -34.9963, -34.9962, -34.9963, -34.9961,\n",
      "        -34.9960, -34.9961, -34.9961, -34.9961, -34.9960, -34.9961, -34.9962,\n",
      "        -34.9960, -34.9960, -34.9960, -34.9960, -34.9963, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9962, -34.9962, -34.9960, -34.9961, -34.9960, -34.9960,\n",
      "        -34.9961, -34.9960, -34.9960, -34.9960, -34.9961, -34.9961, -34.9960,\n",
      "        -34.9960, -34.9961, -34.9962, -34.9961, -34.9960, -34.9960, -34.9961,\n",
      "        -34.9960, -34.9961, -34.9960, -34.9960, -34.9962, -34.9961, -34.9962,\n",
      "        -34.9961, -34.9962, -34.9962, -34.9960, -34.9960, -34.9962, -34.9961,\n",
      "        -34.9961, -34.9961, -34.9961, -34.9960, -34.9960, -34.9960, -34.9961,\n",
      "        -34.9961, -34.9960, -34.9960, -34.9961, -34.9961, -34.9960, -34.9960,\n",
      "        -34.9961, -34.9960, -34.9961, -34.9960, -34.9960, -34.9960, -34.9961,\n",
      "        -34.9961, -34.9960, -34.9960, -34.9976, -34.9972, -34.9981, -34.9960,\n",
      "        -34.9960, -34.9960, -34.9961, -34.9961, -34.9960, -34.9961, -34.9960,\n",
      "        -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9961, -34.9961,\n",
      "        -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9961, -34.9961, -34.9961, -34.9961, -34.9961, -34.9960, -34.9961,\n",
      "        -34.9961, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9960, -34.9961, -34.9961, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9961, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9961, -34.9961, -34.9960, -34.9961, -34.9961, -34.9961, -34.9961,\n",
      "        -34.9961, -34.9961, -34.9961, -34.9961, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9961, -34.9961, -34.9961, -34.9960, -34.9961, -34.9961,\n",
      "        -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9961,\n",
      "        -34.9961, -34.9960, -34.9960, -34.9960, -34.9960, -34.9961, -34.9960,\n",
      "        -34.9961, -34.9961, -34.9960, -34.9961, -34.9960, -34.9961, -34.9960,\n",
      "        -34.9961, -34.9961, -34.9961, -34.9961, -34.9960, -34.9960, -34.9961,\n",
      "        -34.9961, -34.9961, -34.9960, -34.9961, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9961, -34.9961, -34.9961, -34.9960, -34.9961, -34.9961,\n",
      "        -34.9961, -34.9961, -34.9961, -34.9961, -34.9961, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9960, -34.9961, -34.9960, -34.9960, -34.9960, -34.9960,\n",
      "        -34.9960, -34.9961, -34.9961, -34.9962, -35.0004, -35.0039, -34.9960,\n",
      "        -34.9961, -34.9961, -34.9960, -34.9960, -34.9960, -34.9960, -34.9960],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.9778e-03,  3.9778e-03,  3.9779e-03,  ..., -3.9796e-03,\n",
      "         -3.9796e-03, -3.9796e-03],\n",
      "        [ 3.9941e-03,  3.9940e-03,  3.9941e-03,  ..., -3.9945e-03,\n",
      "         -3.9945e-03, -3.9945e-03],\n",
      "        [ 3.9659e-03,  3.9658e-03,  3.9658e-03,  ..., -3.9688e-03,\n",
      "         -3.9689e-03, -3.9689e-03],\n",
      "        ...,\n",
      "        [ 3.9473e-03,  3.9471e-03,  3.9473e-03,  ..., -3.9512e-03,\n",
      "         -3.9512e-03, -3.9512e-03],\n",
      "        [ 3.9525e-03,  3.9524e-03,  3.9525e-03,  ..., -3.9554e-03,\n",
      "         -3.9554e-03,  9.9996e+01],\n",
      "        [ 3.9513e-03,  3.9511e-03,  3.9512e-03,  ..., -3.9538e-03,\n",
      "         -3.9538e-03,  9.9996e+01]], dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 3.9357e-03,  3.9270e-03,  3.9369e-03,  3.9520e-03,  3.9549e-03,\n",
      "         3.9395e-03, -4.4996e+01,  3.9277e-03,  3.9426e-03,  3.9451e-03,\n",
      "         3.9409e-03,  3.9448e-03,  3.9618e-03,  3.9621e-03,  3.9526e-03,\n",
      "         3.9529e-03,  3.9381e-03,  3.9485e-03,  3.9735e-03,  3.9775e-03,\n",
      "         3.9650e-03,  3.9618e-03,  3.9232e-03,  3.9490e-03,  3.9184e-03,\n",
      "         3.9275e-03,  3.9871e-03,  3.9501e-03,  3.9137e-03,  3.9771e-03,\n",
      "         3.9454e-03,  3.9637e-03,  3.8877e-03,  3.9452e-03,  3.9439e-03,\n",
      "         3.9503e-03,  3.9209e-03,  3.9464e-03, -4.4996e+01,  3.9456e-03,\n",
      "        -4.4996e+01,  3.9103e-03,  3.9411e-03,  3.9244e-03,  3.9106e-03,\n",
      "         3.9489e-03,  3.9428e-03,  3.9207e-03,  3.9445e-03,  3.9514e-03,\n",
      "         3.9400e-03,  3.9430e-03,  3.9379e-03,  3.9361e-03,  3.9185e-03,\n",
      "         3.9466e-03,  3.9734e-03,  3.9389e-03,  3.9534e-03,  3.9716e-03,\n",
      "         3.9247e-03, -4.4996e+01,  3.9121e-03,  3.9463e-03,  3.9028e-03,\n",
      "         3.8905e-03,  3.9468e-03,  3.9007e-03,  3.9676e-03,  3.9566e-03,\n",
      "         3.9269e-03,  3.9843e-03,  3.9900e-03,  3.9457e-03,  3.9335e-03,\n",
      "         3.9311e-03, -4.4996e+01,  3.9661e-03,  3.9431e-03,  3.9091e-03,\n",
      "         3.9626e-03,  3.9046e-03,  3.9547e-03,  3.9649e-03,  3.9832e-03,\n",
      "         3.9419e-03,  3.9532e-03,  3.9501e-03,  3.9278e-03,  3.9220e-03,\n",
      "         3.9239e-03,  3.9467e-03,  3.8989e-03,  3.9247e-03,  3.9147e-03,\n",
      "         3.9500e-03,  3.9351e-03,  3.9204e-03,  3.9580e-03,  3.9151e-03,\n",
      "         3.9227e-03,  3.9322e-03,  3.9398e-03,  3.9313e-03,  3.9760e-03,\n",
      "         3.9625e-03,  3.9621e-03,  3.9695e-03,  3.9089e-03,  3.9486e-03,\n",
      "         3.9272e-03,  3.9487e-03,  3.9240e-03,  3.9418e-03,  3.9658e-03,\n",
      "         3.9265e-03,  3.9552e-03,  3.9713e-03,  3.9231e-03,  3.9530e-03,\n",
      "         3.9565e-03,  3.9693e-03,  3.9742e-03,  3.9651e-03,  3.9616e-03,\n",
      "         3.9546e-03,  3.9579e-03,  3.9750e-03,  3.9533e-03,  3.9690e-03,\n",
      "         3.9639e-03,  3.9044e-03,  3.9542e-03,  3.9634e-03,  3.9570e-03,\n",
      "         3.9493e-03,  3.9799e-03,  3.9675e-03,  3.9487e-03,  3.9558e-03,\n",
      "         3.9505e-03,  3.9471e-03,  3.9620e-03,  3.9632e-03,  3.9498e-03,\n",
      "         3.9652e-03,  3.9757e-03,  3.9552e-03,  3.9484e-03,  3.9542e-03,\n",
      "         3.9769e-03,  3.9669e-03,  3.9618e-03,  3.9618e-03,  3.9548e-03,\n",
      "         3.9564e-03,  3.9628e-03,  3.9365e-03,  3.9501e-03,  3.9573e-03,\n",
      "         3.9484e-03,  3.9541e-03,  3.9579e-03, -4.4996e+01, -4.4996e+01,\n",
      "        -4.4996e+01, -4.5002e+01, -4.5002e+01, -4.4996e+01,  3.9396e-03,\n",
      "        -4.4996e+01,  3.9476e-03,  3.9639e-03,  3.9557e-03,  3.9451e-03,\n",
      "         3.9402e-03,  3.9430e-03,  3.9549e-03,  3.9555e-03,  3.9611e-03,\n",
      "         3.9853e-03,  3.9504e-03,  3.9483e-03,  3.9494e-03,  3.9444e-03,\n",
      "         3.9485e-03,  3.9505e-03,  3.9477e-03,  3.9487e-03,  3.9178e-03,\n",
      "         3.9529e-03,  3.9460e-03,  3.9624e-03,  3.9612e-03,  3.9478e-03,\n",
      "         3.9558e-03,  3.9576e-03,  3.9480e-03,  3.9191e-03,  3.9221e-03,\n",
      "         3.9359e-03,  3.9611e-03,  3.9652e-03,  3.9743e-03,  3.9585e-03,\n",
      "         3.9578e-03,  3.9484e-03,  3.9328e-03,  3.9517e-03,  3.9359e-03,\n",
      "         3.9362e-03,  3.9339e-03,  3.9504e-03,  3.9566e-03,  3.9481e-03,\n",
      "         3.9235e-03, -4.4996e+01,  3.9415e-03,  3.9358e-03,  3.9374e-03,\n",
      "         3.9325e-03,  3.9629e-03,  3.9303e-03,  3.9487e-03,  3.9548e-03,\n",
      "         2.4323e-03,  3.9591e-03,  3.9578e-03,  3.9527e-03,  3.9475e-03,\n",
      "         3.9142e-03,  3.9245e-03,  3.9602e-03,  3.9678e-03,  3.9626e-03,\n",
      "         3.9689e-03,  3.9739e-03,  3.9600e-03,  3.9711e-03,  3.9420e-03,\n",
      "         3.9774e-03,  3.9906e-03,  3.9684e-03,  3.9802e-03,  3.9678e-03,\n",
      "         3.9614e-03,  3.9602e-03,  3.9686e-03,  3.9607e-03,  3.9539e-03,\n",
      "         3.9530e-03,  3.9549e-03,  3.9414e-03,  3.9596e-03,  3.9538e-03,\n",
      "         3.9611e-03,  3.9515e-03,  3.9531e-03,  3.9525e-03,  3.9548e-03,\n",
      "         3.9535e-03,  3.9464e-03,  3.9505e-03,  3.9717e-03,  3.9617e-03,\n",
      "         3.9569e-03,  3.9497e-03, -4.4996e+01, -4.4996e+01, -4.4996e+01,\n",
      "        -4.4996e+01, -4.4996e+01, -4.4996e+01, -4.4996e+01, -4.4996e+01,\n",
      "        -4.4996e+01, -4.4996e+01, -4.4996e+01, -4.4996e+01, -4.4996e+01],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.1636e-03, -3.9795e-03,  3.9782e-03,  ..., -3.9793e-03,\n",
      "         -3.9794e-03, -3.9795e-03],\n",
      "        [ 3.9945e-03,  3.4833e-03,  3.9947e-03,  ...,  3.9916e-03,\n",
      "         -3.9937e-03, -3.9941e-03],\n",
      "        [-3.9652e-03, -3.9684e-03,  3.0623e-03,  ..., -3.9682e-03,\n",
      "         -3.9684e-03, -3.9685e-03],\n",
      "        ...,\n",
      "        [ 3.9499e-03, -3.9017e-03,  3.9504e-03,  ..., -1.4536e-03,\n",
      "          9.9996e+01, -3.9494e-03],\n",
      "        [ 3.9547e-03,  3.9510e-03,  3.9550e-03,  ...,  1.0000e+02,\n",
      "          1.4294e-03,  9.9996e+01],\n",
      "        [ 3.9533e-03,  3.9514e-03,  3.9535e-03,  ...,  3.9522e-03,\n",
      "          1.0000e+02,  1.3559e-04]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2723709-1869-4e8c-9e30-ce43f623a148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
