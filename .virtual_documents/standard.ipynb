import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import json
from sklearn.model_selection import train_test_split
import torch
from torch import nn, optim
import time


# Load data

with open("data/2022-08-02-01.json", "r") as file:
    data_text = file.read()
    data_dict = json.loads(data_text)
    data = np.array(data_dict["trace_array"])


plt.plot(data[15])


152 * 20


class SimpleNetwork(nn.Module):
    def __init__(self):
        super().__init__()

        self.first_layer = nn.Linear(3040, 1000)
        self.first_act = nn.Sigmoid()
        self.second_layer = nn.Linear(1000, 500)
        self.second_act = nn.Sigmoid()
        self.third_layer = nn.Linear(500, 250)
        self.third_act = nn.Sigmoid()

        self.process = nn.Sequential(self.first_layer,
                                     self.first_act,
                                     self.second_layer,
                                     self.second_act,
                                     self.third_layer,
                                     self.third_act)
        
        self.final_layer = nn.Linear(250, 152)

    def forward(self, x):
        x = torch.reshape(x, (x.shape[0], 3040))
        x = self.process(x)
        x = self.final_layer(x)
        return x


data.shape


# Window Sample

simple_x = []
simple_y = []

for i in range(1580):
    window = data[:, i:i+20]
    simple_x.append(window)
    simple_y.append(data[:, i+20])

simple_x = np.array(simple_x)
simple_y = np.array(simple_y)


x_train, x_test, y_train, y_test = train_test_split(simple_x, simple_y, test_size=0.2, random_state=5, shuffle=True)


x_train.shape, y_train.shape


def train(model, criterion, optimizer, points, labels, test_x, test_y, epoches=5, do_print=True):
    
    total_start_time = time.time()
    loss = -1
    for i in range(1,epoches+1):
        start_time = time.time()
        optimizer.zero_grad()
        y = model(points)
        loss = criterion(y, labels)
        loss.backward()
        optimizer.step()
        end_time = time.time()
        if(i % 50 == 0 and do_print):
            test_out = model(test_x)
            test_loss = criterion(test_out, test_y)
            print(f"{i}: Loss {loss.item()}, Test Loss {test_loss.item()}")
            
    total_end_time = time.time()
    total_time_taken = total_end_time - total_start_time
    if(do_print):
        print(f"Total Time {total_time_taken}")
        
    final_loss = loss.item()
    
    return (total_time_taken, final_loss)


# Training Setup

torch.manual_seed(4687)
model = SimpleNetwork()

optimiser = optim.Adam(model.parameters(), lr=0.001)
crit = nn.MSELoss()

x_train_tensor = torch.from_numpy(x_train).float()
y_train_tensor = torch.from_numpy(y_train).float()

test_tensor = torch.from_numpy(x_test).float()
test_y_tensor = torch.from_numpy(y_test).float()

x_train_tensor.shape[0]


time_taken, final_loss = train(model, crit, optimiser, x_train_tensor, y_train_tensor, test_tensor, test_y_tensor, epoches=5000, do_print=True)


x_test_tensor = torch.from_numpy(simple_x).float()
output = model(x_test_tensor).detach().numpy()


plt.plot(simple_y[:, 0])
plt.plot(output[:, 0])


output = model(test_tensor)


crit(test_y_tensor, output)


# Far seeing / Close seeing

# Set window to 0 or -1 for different effect
window = simple_x[0]
window = window.reshape(1, 152, 20)

line = []

for i in range(1000):
    input = torch.from_numpy(window).float()
    out = model(input).detach().numpy().reshape(1, 152, 1)
    line.append(out)
    window = np.concatenate((window[:, :, 1:], out), axis=2) 

line = np.array(line).reshape(1000, 152)
ext_line = np.concatenate((simple_x[0].T, line), axis = 0).T
combined = np.concatenate((data.T, line), axis = 0).T
line = line.T


plt.plot(data[60, :1000])
plt.plot(ext_line[60])


text = '''50: Loss 0.542038083076477, Test Loss 0.5256966948509216
100: Loss 0.4054592549800873, Test Loss 0.3994710445404053
150: Loss 0.31234702467918396, Test Loss 0.31763526797294617
200: Loss 0.2550389766693115, Test Loss 0.2693379819393158
250: Loss 0.21755757927894592, Test Loss 0.24038328230381012
300: Loss 0.18935471773147583, Test Loss 0.22041144967079163
350: Loss 0.1665990799665451, Test Loss 0.2061326652765274
400: Loss 0.14783909916877747, Test Loss 0.19608989357948303
450: Loss 0.13229107856750488, Test Loss 0.1895163506269455
500: Loss 0.11910996586084366, Test Loss 0.18527695536613464
550: Loss 0.10766829550266266, Test Loss 0.1821182519197464
600: Loss 0.0978560596704483, Test Loss 0.18114329874515533
650: Loss 0.08961444348096848, Test Loss 0.18042030930519104
700: Loss 0.08156861364841461, Test Loss 0.17997589707374573
750: Loss 0.07496180385351181, Test Loss 0.18021813035011292
800: Loss 0.06915687769651413, Test Loss 0.18081150949001312
850: Loss 0.0639190673828125, Test Loss 0.18156787753105164
900: Loss 0.05974731221795082, Test Loss 0.1831955760717392
950: Loss 0.05518801510334015, Test Loss 0.18331597745418549
1000: Loss 0.051656026393175125, Test Loss 0.18429192900657654
1050: Loss 0.04848547279834747, Test Loss 0.18667685985565186
1100: Loss 0.04552420973777771, Test Loss 0.18853512406349182
1150: Loss 0.04479101672768593, Test Loss 0.19013947248458862
1200: Loss 0.04012039303779602, Test Loss 0.19091692566871643
1250: Loss 0.04015333577990532, Test Loss 0.19104483723640442
1300: Loss 0.035501331090927124, Test Loss 0.19275297224521637
1350: Loss 0.03422030061483383, Test Loss 0.19337064027786255
1400: Loss 0.031919099390506744, Test Loss 0.19534169137477875
1450: Loss 0.03047347441315651, Test Loss 0.19641759991645813
1500: Loss 0.03293424844741821, Test Loss 0.20257531106472015
1550: Loss 0.027387991547584534, Test Loss 0.19908273220062256
1600: Loss 0.026105791330337524, Test Loss 0.20012858510017395
1650: Loss 0.025347387418150902, Test Loss 0.20099873840808868
1700: Loss 0.02372319996356964, Test Loss 0.20310086011886597
1750: Loss 0.023021327331662178, Test Loss 0.20501384139060974
1800: Loss 0.02626599185168743, Test Loss 0.20874105393886566
1850: Loss 0.021335197612643242, Test Loss 0.20615136623382568
1900: Loss 0.02012675255537033, Test Loss 0.20741599798202515
1950: Loss 0.019542086869478226, Test Loss 0.20887860655784607
2000: Loss 0.018368931487202644, Test Loss 0.20932257175445557
2050: Loss 0.018004706129431725, Test Loss 0.2103928178548813
2100: Loss 0.016851935535669327, Test Loss 0.21110552549362183
2150: Loss 0.01688103750348091, Test Loss 0.21340903639793396
2200: Loss 0.017057420685887337, Test Loss 0.21755096316337585
2250: Loss 0.014956419356167316, Test Loss 0.21410110592842102
2300: Loss 0.014368653297424316, Test Loss 0.21486437320709229
2350: Loss 0.013967890292406082, Test Loss 0.21615070104599
2400: Loss 0.013438957743346691, Test Loss 0.21673834323883057
2450: Loss 0.012884494848549366, Test Loss 0.2174578458070755
2500: Loss 0.012454218231141567, Test Loss 0.21820470690727234
2550: Loss 0.012308037839829922, Test Loss 0.21949820220470428
2600: Loss 0.011623058468103409, Test Loss 0.21994024515151978
2650: Loss 0.011296684853732586, Test Loss 0.22038042545318604
2700: Loss 0.011610870249569416, Test Loss 0.22097040712833405
2750: Loss 0.010456967167556286, Test Loss 0.22183865308761597
2800: Loss 0.010243723168969154, Test Loss 0.22221875190734863
2850: Loss 0.010418234393000603, Test Loss 0.22271780669689178
2900: Loss 0.00989307276904583, Test Loss 0.22457635402679443
2950: Loss 0.010449792258441448, Test Loss 0.2251722663640976
3000: Loss 0.008916887454688549, Test Loss 0.22476594150066376
3050: Loss 0.008775592781603336, Test Loss 0.22409193217754364
3100: Loss 0.008753863163292408, Test Loss 0.22510497272014618
3150: Loss 0.008634831756353378, Test Loss 0.2250356823205948
3200: Loss 0.007914298214018345, Test Loss 0.22645343840122223
3250: Loss 0.007886702194809914, Test Loss 0.2267347276210785
3300: Loss 0.008118726313114166, Test Loss 0.22753895819187164
3350: Loss 0.009784420020878315, Test Loss 0.22915062308311462
3400: Loss 0.007058246992528439, Test Loss 0.22795610129833221
3450: Loss 0.006872599013149738, Test Loss 0.2280198037624359
3500: Loss 0.006854375824332237, Test Loss 0.22836509346961975
3550: Loss 0.010164786130189896, Test Loss 0.23077292740345
3600: Loss 0.00629142252728343, Test Loss 0.22964805364608765
3650: Loss 0.006227181758731604, Test Loss 0.22963695228099823
3700: Loss 0.005979273002594709, Test Loss 0.2300928682088852
3750: Loss 0.0059727695770561695, Test Loss 0.23048610985279083
3800: Loss 0.005928196012973785, Test Loss 0.23175646364688873
3850: Loss 0.00572794396430254, Test Loss 0.23137886822223663
3900: Loss 0.00673326849937439, Test Loss 0.23310096561908722
3950: Loss 0.005233382806181908, Test Loss 0.2318723499774933
4000: Loss 0.005143740214407444, Test Loss 0.23195071518421173
4050: Loss 0.005550108850002289, Test Loss 0.23160859942436218
4100: Loss 0.005232494790107012, Test Loss 0.23407752811908722
4150: Loss 0.004770148545503616, Test Loss 0.2336321324110031
4200: Loss 0.004630685318261385, Test Loss 0.23324871063232422
4250: Loss 0.004620205610990524, Test Loss 0.2334575057029724
4300: Loss 0.004485914018005133, Test Loss 0.23385225236415863
4350: Loss 0.004583494272083044, Test Loss 0.23414236307144165
4400: Loss 0.004239405505359173, Test Loss 0.23501554131507874
4450: Loss 0.004084727726876736, Test Loss 0.2350863218307495
4500: Loss 0.004201652016490698, Test Loss 0.23499487340450287
4550: Loss 0.00421117665246129, Test Loss 0.235922709107399
4600: Loss 0.004282624460756779, Test Loss 0.23707522451877594
4650: Loss 0.0037030859384685755, Test Loss 0.23631535470485687
4700: Loss 0.0037389961071312428, Test Loss 0.23633074760437012
4750: Loss 0.003720927983522415, Test Loss 0.23700933158397675
4800: Loss 0.003497577039524913, Test Loss 0.23709425330162048
4850: Loss 0.0035504538100212812, Test Loss 0.23719997704029083
4900: Loss 0.003501272527500987, Test Loss 0.23780807852745056
4950: Loss 0.003465791931375861, Test Loss 0.23767077922821045
5000: Loss 0.003151476150378585, Test Loss 0.23813748359680176'''


split_lines = text.split("\n")


split = [a.split("Loss") for a in split_lines]


split


numbers = [ a[1:] for a in split]


numbers = [ [float(a.replace(", Test", "").strip()) for a in b] for b in numbers]


numbers = np.array(numbers)


numbers[:, 0]


plt.plot(numbers[:, 0])
plt.plot(numbers[:, 1])
